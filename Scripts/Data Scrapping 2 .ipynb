{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import lxml.html\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / size)     \n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slobodna Evropa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data structure we need to load more content on the page for a maximum of three times -> extract all links and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_2018 = ['11','12']\n",
    "months_2019 = ['1','2']\n",
    "days_1 = ['1','2','3','4','5','6','7','8','9']\n",
    "days_2 = range(10,32)\n",
    "days_2 = [\"{:02d}\".format(x) for x in days_2]\n",
    "days = days_1 + days_2\n",
    "categories = ['500', '504']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "se_urls_2018 = []\n",
    "for a in categories:\n",
    "    for i in months_2018:\n",
    "        for y in log_progress(days):\n",
    "            try: \n",
    "                url = 'https://www.slobodnaevropa.org/z/' + a + '/2018/' + i + '/' + y\n",
    "                driver.get(url)\n",
    "                python_button = driver.find_element_by_css_selector('.btn__text')\n",
    "                count = 3\n",
    "                while count > 1:\n",
    "                    python_button.click()\n",
    "                    count -= 1\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^/a/\")}):\n",
    "                    se_urls_2018.append(link.get('href'))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_urls = se_urls_2018 + se_urls_2019\n",
    "clean_links = ['https://www.slobodnaevropa.org' + x for x in se_urls]\n",
    "clean_links = list(set(clean_links))\n",
    "len(clean_links)\n",
    "np.save(\"clean_links_se.npy\", clean_links)\n",
    "clean_links = np.load(\"clean_links_se.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "se_urls_2019 = []\n",
    "for a in categories:\n",
    "    for i in months_2019:\n",
    "        for y in log_progress(days):\n",
    "            try: \n",
    "                url = 'https://www.slobodnaevropa.org/z/' + a + '/2019/' + i + '/' + y\n",
    "                driver.get(url)\n",
    "                python_button = driver.find_element_by_css_selector('.btn__text')\n",
    "                count = 3\n",
    "                while count > 1:\n",
    "                    python_button.click()\n",
    "                    count -= 1\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^/a/\")}):\n",
    "                    se_urls_2019.append(link.get('href'))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for page in log_progress(clean_links): \n",
    "    new_pages = requests.get(page)\n",
    "    pages.append(new_pages)\n",
    "trees = [page.content for page in pages]\n",
    "soups = [BeautifulSoup(tree, 'lxml') for tree in trees]\n",
    "titles = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        title = soup.find(\"div\", {\"class\" : \"col-title col-xs-12 col-md-10 pull-right\"}).findAll('h1')\n",
    "        titles.append(title)\n",
    "    except AttributeError:\n",
    "        titles.append('problem')\n",
    "texts = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        text = soup.find(\"div\", {\"class\" : \"col-xs-12 col-sm-12 col-md-8 col-lg-8 pull-left bottom-offset content-offset\"}).findAll('p')\n",
    "        texts.append(text)\n",
    "    except AttributeError:\n",
    "        texts.append('problem')\n",
    "dates = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        date = soup.find(\"div\", {\"class\" : \"published\"}).findAll('time')\n",
    "        dates.append(date)\n",
    "    except AttributeError:\n",
    "        dates.append('problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "date     0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1794"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(\n",
    "    {'title': pd.Series(titles),\n",
    "     'date' : pd.Series(dates),\n",
    "     'text': pd.Series(texts),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('se.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_2018 = ['11','12']\n",
    "months_2019 = ['1','2']\n",
    "days_1 = ['1','2','3','4','5','6','7','8','9']\n",
    "days_2 = range(10,32)\n",
    "days_2 = [\"{:02d}\".format(x) for x in days_2]\n",
    "days = days_1 + days_2\n",
    "categories = ['2086', '2079', '2087', '2080', '2078', '2084']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "voa_urls_2018 = []\n",
    "for a in categories:\n",
    "    for i in months_2018:\n",
    "        for y in log_progress(days):\n",
    "            try: \n",
    "                url = 'https://www.glasamerike.net/z/' + a + '/2018/' + i + '/' + y\n",
    "                driver.get(url)\n",
    "                python_button = driver.find_element_by_css_selector('.btn__text')\n",
    "                count = 3\n",
    "                while count > 1:\n",
    "                    python_button.click()\n",
    "                    count -= 1\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^/a/\")}):\n",
    "                    voa_urls_2018.append(link.get('href'))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "voa_urls_2019 = []\n",
    "for a in categories:\n",
    "    for i in months_2019:\n",
    "        for y in log_progress(days):\n",
    "            try: \n",
    "                url = 'https://www.glasamerike.net/z/' + a + '/2019/' + i + '/' + y\n",
    "                driver.get(url)\n",
    "                python_button = driver.find_element_by_css_selector('.btn__text')\n",
    "                count = 3\n",
    "                while count > 1:\n",
    "                    python_button.click()\n",
    "                    count -= 1\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                for link in soup.findAll('a', attrs={'href': re.compile(\"^/a/\")}):\n",
    "                    voa_urls_2019.append(link.get('href'))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"voa_2018links.npy\", voa_urls_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voa_urls = voa_urls_2018 + voa_urls_2019\n",
    "clean_links = ['https://www.glasamerike.net' + x for x in voa_urls]\n",
    "clean_links = list(set(clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"voa_clean_links.npy\", clean_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for page in log_progress(clean_links): \n",
    "    new_pages = requests.get(page)\n",
    "    pages.append(new_pages)\n",
    "trees = [page.content for page in pages]\n",
    "soups = [BeautifulSoup(tree, 'lxml') for tree in trees]\n",
    "\n",
    "titles = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        title = soup.find(\"div\", {\"class\" : \"col-title col-xs-12 col-md-10 pull-right\"}).findAll('h1')\n",
    "        titles.append(title)\n",
    "    except AttributeError:\n",
    "        titles.append(\"problem\")\n",
    "texts = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        text = soup.find(\"div\", {\"class\" : \"col-xs-12 col-sm-12 col-md-8 col-lg-8 pull-left bottom-offset content-offset\"}).findAll('p')\n",
    "        texts.append(text)\n",
    "    except AttributeError:\n",
    "        texts.append(\"problem\")\n",
    "dates = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        date = soup.find(\"div\", {\"class\" : \"published\"}).findAll('time')\n",
    "        dates.append(date)\n",
    "    except AttributeError:\n",
    "        dates.append(\"problem\")\n",
    "categories = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        category = soup.find(\"div\", {\"class\" : \"category\"})\n",
    "        categories.append(category)\n",
    "    except AttributeError:\n",
    "        categories.append(\"problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(\n",
    "    {'title': pd.Series(titles),\n",
    "     'date' : pd.Series(dates),\n",
    "     'text': pd.Series(texts),\n",
    "     'category' : pd.Series(categories),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "date        0\n",
       "text        0\n",
       "category    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Category does not throw an Attribute Error it returns seven missing values where all others return \"problem\". We just drop those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[pd.notnull(all_data['category'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('voa.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_urls = []\n",
    "url = 'http://rs.n1info.com/Svet'\n",
    "driver.get(url)\n",
    "try: \n",
    "    i = 0 \n",
    "    while i < 80:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        python_button = driver.find_element_by_css_selector('.js-btn-load-more')\n",
    "        python_button.click()\n",
    "        time.sleep(3)\n",
    "        if i == 80:\n",
    "            break \n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "            \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/Svet/\")}):\n",
    "    n1_urls.append(link.get('href'))\n",
    "    \n",
    "url = 'http://rs.n1info.com/Region'\n",
    "driver.get(url)\n",
    "try: \n",
    "    i = 0 \n",
    "    while i < 30:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        python_button = driver.find_element_by_css_selector('.js-btn-load-more')\n",
    "        python_button.click()\n",
    "        time.sleep(3)\n",
    "        if i == 30:\n",
    "            break \n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "            \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/Region/\")}):\n",
    "    n1_urls.append(link.get('href'))\n",
    "\n",
    "url = 'http://rs.n1info.com/Arhiva-Vesti'\n",
    "driver.get(url)\n",
    "try: \n",
    "    i = 0 \n",
    "    while i < 100:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        python_button = driver.find_element_by_css_selector('.js-btn-load-more')\n",
    "        python_button.click()\n",
    "        time.sleep(3)\n",
    "        if i == 100:\n",
    "            break \n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "            \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/Vesti/\")}):\n",
    "    n1_urls.append(link.get('href'))\n",
    "    \n",
    "url = 'http://rs.n1info.com/Kultura'\n",
    "driver.get(url)\n",
    "try: \n",
    "    i = 0 \n",
    "    while i < 25:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        python_button = driver.find_element_by_css_selector('.js-btn-load-more')\n",
    "        python_button.click()\n",
    "        time.sleep(3)\n",
    "        if i == 25:\n",
    "            break \n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "            \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/Kultura/\")}):\n",
    "    n1_urls.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n1_urls)\n",
    "np.save('n1_urls.npy', n1_urls)\n",
    "np.save('copy_n1_urls.npy', copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "svet = [s for s in n1_urls if \"/Svet/\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = [s for s in n1_urls if \"/Region/\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kultura = [s for s in n1_urls if \"/Kultura/\" in s]\n",
    "len(kultura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5790"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vesti = [s for s in n1_urls if \"/Vesti/\" in s]\n",
    "len(vesti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vesti = [s for s in n1_urls if \"/Vesti/\" in s]\n",
    "len(vesti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3058"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(vesti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for page in log_progress(clean_links): \n",
    "    new_pages = requests.get(page)\n",
    "    pages.append(new_pages)\n",
    "trees = [page.content for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = []\n",
    "for tree in log_progress(trees):\n",
    "    try: \n",
    "        soup = BeautifulSoup(tree, 'lxml')\n",
    "        soups.append(soup)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        title = soup.find(\"div\", {\"class\" : \"single-article-header\"}).findAll('h1')\n",
    "        titles.append(title)\n",
    "    except AttributeError:\n",
    "        titles.append(\"problem\")\n",
    "        \n",
    "texts = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        text = soup.find(\"div\", {\"class\" : \"single-article-content js_resizeFont\"}).findAll('p')\n",
    "        texts.append(text)\n",
    "    except AttributeError:\n",
    "        texts.append(\"problem\")\n",
    "\n",
    "categories = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        category = soup.find(\"div\", {\"class\" : \"article-info-cnt\"}).findAll('p')\n",
    "        categories.append(category)\n",
    "    except AttributeError:\n",
    "        categories.append(\"problem\")\n",
    "        \n",
    "dates = []\n",
    "for soup in log_progress(soups):\n",
    "    try:\n",
    "        date = soup.find(\"span\", {\"class\" : \"date\"})\n",
    "        dates.append(date)\n",
    "    except AttributeError:\n",
    "        categories.append(\"problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(\n",
    "    {'title': titles,\n",
    "     'date' : dates,\n",
    "     'text': texts,\n",
    "     'category' : categories\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('n1.csv', sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
