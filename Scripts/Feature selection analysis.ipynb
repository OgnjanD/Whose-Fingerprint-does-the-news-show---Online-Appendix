{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Sbk79o2mBVeZ",
    "outputId": "9a31d6e1-ce3b-46cf-9060-6a0287fc1fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import ranksums\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-WbDxSkBH_G"
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('complete_dataset.csv', sep = ',')\n",
    "all_data.columns\n",
    "all_data['country-source'].value_counts()\n",
    "all_data = all_data.rename(columns = {'country-source':'source'})\n",
    "all_data.source = all_data.source.replace({'USA': 0, 'RU': 1})\n",
    "\n",
    "featuress = all_data['ner'].tolist()\n",
    "clean_ner = []\n",
    "import re\n",
    "for x in featuress:\n",
    "  x = re.sub(r'[^\\w ]', '', x)\n",
    "  clean_ner.append(x)\n",
    "y = np.array(all_data['source'].values)\n",
    "featuress = all_data['ner'].tolist()\n",
    "clean_ner = []\n",
    "import re\n",
    "for x in featuress:\n",
    "  x = re.sub(r'[^\\w ]', '', x)\n",
    "  clean_ner.append(x)\n",
    "  \n",
    "loaded_vec = joblib.load('gen_1_fit.pkl')\n",
    "count = loaded_vec.transform(clean_ner)\n",
    "\n",
    "#https://stackoverflow.com/questions/47665176/converting-a-pandas-dataframe-to-a-scipy-sparse-matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "features_n = all_data[['article_len','ratio_not_stop_w','ratio_not_stop_w_t','avg_word_len', 'title_len', 'ratio_unique_words','A','B','D', 'E', 'F']]\n",
    "\n",
    "\n",
    "m = scipy.sparse.hstack((t2, count))\n",
    "y = np.array(all_data['source'].values)\n",
    "X = np.array(features_n.values)\n",
    "xg_train, xg_test, yg_train, yg_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVO7IZVI1w3O"
   },
   "source": [
    "#Univariate selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "hnGPb9TQ10vj",
    "outputId": "2e819ff6-3587-4901-b8bc-88f8cd131bda"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-42117d2d651d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_len'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ratio_not_stop_w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ratio_not_stop_w_t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'avg_word_len'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title_len'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ratio_unique_words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'E'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstd_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "np.set_printoptions(suppress=True)\n",
    "features_n = all_data[['article_len','ratio_not_stop_w','ratio_not_stop_w_t','avg_word_len', 'title_len', 'ratio_unique_words','A','B','D', 'E', 'F']]\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, y)\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=4)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:4,:])\n",
    "#most relevenat feature identified by this analysis are article lenth, decisively most relevant, followed by title length, followed by frame A (human interest) followed by Frame E (anti-west)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HM5iyP9gdmw"
   },
   "source": [
    "#Recursive feature variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Y2EFMzAmjUKo",
    "outputId": "e8c9a2ba-aef9-46c4-f917-30db9cfd5646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([91.    ,  0.1648,  0.    ,  7.0989,  6.    ,  0.7912,  0.    ,\n",
       "        1.    ,  0.    ,  0.    ,  0.    ])"
      ]
     },
     "execution_count": 127,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "OEP2QLsjgcvO",
    "outputId": "fd49392e-9f5b-4a21-8866-ebf9fdbb4a19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([91.    ,  7.0989,  6.    ,  0.    ,  1.    ,  0.    ,  0.    ,\n",
       "        0.    ])"
      ]
     },
     "execution_count": 128,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new[1]\n",
    "#0, 3, 4, 6,7,8,9, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbHVRQj8gFGs"
   },
   "source": [
    "#Variance threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "r3EBfItjeyAu",
    "outputId": "6df1f0e8-0e9f-4eb4-ff5e-d7d150292380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 242.    ,    6.6901,    7.    ,    1.    ],\n",
       "       [  91.    ,    7.0989,    6.    ,    1.    ],\n",
       "       [ 105.    ,    5.9048,    7.    ,    1.    ],\n",
       "       ...,\n",
       "       [  97.    ,    6.3711,    7.    ,    0.    ],\n",
       "       [1850.    ,    6.1108,    7.    ,    1.    ],\n",
       "       [1191.    ,    6.4467,    7.    ,    1.    ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "m = sel.fit_transform(X)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qA5nLLgyIQN4"
   },
   "source": [
    "#Feature Importance with Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ic1Tb7v3AhR9",
    "outputId": "e788a056-3113-4047-de27-19f9cc39ced5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1696 0.1718 0.0928 0.2098 0.1278 0.1653 0.0219 0.0142 0.0099 0.0104\n",
      " 0.0064]\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=5000)\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrohjtzrbyHm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Feature reduction analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
