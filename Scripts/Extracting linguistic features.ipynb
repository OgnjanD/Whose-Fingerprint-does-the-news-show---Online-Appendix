{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LenovoPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import krippendorff as k\n",
    "import numpy as np\n",
    "import collections\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "import import_ipynb\n",
    "import SerbStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "from SerbStemmer import stem_str\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import transliterate\n",
    "from transliterate import translit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER and Sentiment was obtaioned via the transliterate package which can only run on a non-Windows OS. The code for this part of the analysis was run in a virtual machine, but is presented here. Do not run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text, Word\n",
    "import transliterate\n",
    "from transliterate import translit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stopword list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting stopwords from all data to create specialized stopwords list. Read in all of our data, use both titles and texts. Place all in flat list with lowercase. Get top 200 and clean up manually removing words such as \"Russia\", \"Serbia\", \"Trump\", \"Vucic\".\n",
    "\n",
    "Also take stopwords from https://github.com/Xangis/extra-stopwords/blob/master/serbian. Clean up both lists and drop duplicates. This is our stopword list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vostok = pd.read_csv('vostok_clean.csv', sep = ',')\n",
    "sputnik = pd.read_csv('sputnik_clean.csv', sep = ',')\n",
    "n1 = pd.read_csv('n1_clean.csv', sep = ',')\n",
    "voa = pd.read_csv('voa_clean.csv', sep = ',')\n",
    "rfe = pd.read_csv('rfe_clean.csv', sep = ',')\n",
    "all_data = pd.concat([vostok,sputnik, n1, voa, rfe], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = all_data['title'].tolist()\n",
    "text = all_data['text'].tolist()\n",
    "all_text = titles+text\n",
    "flat_list = []\n",
    "for sublist in all_text:\n",
    "    for word in sublist.split():\n",
    "        flat_list.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(flat_list)\n",
    "stopword_filter = []\n",
    "for word in counter.most_common(200):\n",
    "    stopword_filter.append(word)\n",
    "stopword_filter = pd.DataFrame(stopword_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_list = pd.read_csv(\"Git_stop_list.csv\")\n",
    "git_list = pd.DataFrame(git_list)\n",
    "git_list['ba?;'] = git_list['ba?;'].str.replace('?', 'ć').str.replace(';', '')\n",
    "git_list['ba?;'] = git_list['ba?;'].astype(str)\n",
    "git_list.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_filter = pd.DataFrame(stopword_filter[0])\n",
    "stopword_filter.columns = ['words']\n",
    "all_stop = stopword_filter.append(git_list, ignore_index = True)\n",
    "all_stop = all_stop.drop_duplicates()\n",
    "all_stop = all_stop.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop['words'][206] = all_stop['words'][206].replace('doćao', 'došao')\n",
    "all_stop['words'][226] = all_stop['words'][226].replace('joć', 'još')\n",
    "all_stop['words'][241] = all_stop['words'][241].replace('nećto', 'nešto')\n",
    "all_stop['words'][245] = all_stop['words'][245].replace('nićta', 'ništa')\n",
    "all_stop['words'][255] = all_stop['words'][255].replace('otićao', 'otišao')\n",
    "all_stop['words'][266] = all_stop['words'][266].replace('ćta', 'šta')\n",
    "all_stop['words'][283] = all_stop['words'][283].replace('vać', 'vaš')\n",
    "all_stop['words'][287] = all_stop['words'][287].replace('viće', 'više')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep relevant words from list\n",
    "all_stop = all_stop.drop([14, 27, 35, 47, 49, 62, 77, 79, 86, 92, 93,94, 96, 103, 105, 120, 125, 127, 130, 132, 133, 134, 137, 140, 141, 154, 155, 158, 160, 165, 166, 171, 172, 173, 178, 181, 190,196,198, 289, 290, 291])\n",
    "all_stop.to_csv('stopwords.csv', encoding = 'utf-8', index = False )\n",
    "stopwords = pd.read_csv('stopwords.csv')\n",
    "stopwords = stopwords['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_1 = pd.read_csv('coder_1_data.csv', sep = ',')\n",
    "coder_4 = pd.read_csv(\"coder_4_data.csv\", sep =',')\n",
    "coder_3 = pd.read_csv('coder_3_data.csv', sep = ',')\n",
    "coder_5 = pd.read_csv('coder_5_data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are extracting the automated features from all of the texts. These are the following: \n",
    "Headline Length \n",
    "Article Length \n",
    "Average Word Length \n",
    "Language complexity (proportion of unique words to all words)\n",
    "Ratio of stopwords to all words \n",
    "Named Entity recognition (title and text) \n",
    "Top five words after stopwords (basic topic modelling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_features(name):\n",
    "    name['text'] = name['text'].str.replace(',', '').str.replace('<br/>', '')\n",
    "    s = stopwords\n",
    "    name['article_len'] = name['text'].str.split().apply(len)#  length of text (no. of words)\n",
    "    name['string_len'] = name['text'].apply(len)#length of total string (characters)\n",
    "    name['stop_words'] = [list(set(x).intersection(s)) for x in name['text'].str.split()] # stopwords\n",
    "    name['stop_words_t'] = [list(set(x).intersection(s)) for x in name['title'].str.split()] # stopwords\n",
    "    name['title_len'] = name['title'].str.split().apply(len) # no of words in title = title length\n",
    "    name['no_stop_words'] = name['stop_words'].str.len()# number of stopwords number\n",
    "    name['ratio_not_stop_w'] = name['no_stop_words']/name['article_len']\n",
    "    name['title_len'] = name['title'].str.split().apply(len)\n",
    "    name['no_stop_words_t'] = name['stop_words_t'].str.len()# number of stopwords number\n",
    "    name['ratio_not_stop_w_t'] = name['no_stop_words_t']/name['title_len']\n",
    "    name['avg_word_len'] = name['string_len']/name['article_len'] #derived avg. word length\n",
    "    name['wordscounter'] = name['text'].str.lower().str.split().apply(set).apply(len) #no of unique words\n",
    "    name['ratio_unique_words'] = name['wordscounter']/name['article_len'] #prop of unique word to total words = language complexity\n",
    "    del name['wordscounter']\n",
    "    del name['string_len']\n",
    "    del name['no_stop_words']\n",
    "    del name['stop_words']\n",
    "    del name['stop_words_t']\n",
    "    del name['no_stop_words_t']\n",
    "    return name\n",
    "  \n",
    "\n",
    "def ner(name):\n",
    "    tekstovi = name['text'].tolist()\n",
    "    tekstovi = [translit(tekst, 'sr', reversed = False) for tekst in tekstovi]\n",
    "    tekstovi = [Text(tekst, hint_language_code = 'sr') for tekst in tekstovi]\n",
    "    tekstovi = [tekst.entities for tekst in tekstovi]\n",
    "    titlovi = name['title'].tolist()\n",
    "    titlovi = [translit(titl, 'sr', reversed = False) for titl in titlovi]\n",
    "    titlovi = [Text(titl, hint_language_code = 'sr') for titl in titlovi]\n",
    "    titlovi = [titl.entities for titl in titlovi]\n",
    "    all_ner = list(zip(titlovi, tekstovi)) \n",
    "    ner = []\n",
    "    for line in all_ner:\n",
    "        inner_ner = []\n",
    "        for lines in line:\n",
    "            for element in lines:\n",
    "                for el in element:\n",
    "                    inner_ner.append(el)\n",
    "        ner.append(inner_ner)\n",
    "    ner = [list(set(line)) for line in ner] #return only unique values \n",
    "    ner = [[translit(x, 'sr', reversed = True) for x in ner_value] for ner_value in ner]\n",
    "    ner = [[x.lower() for x in ner_value] for ner_value in ner]\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run the NER and Sen functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_1_features = automated_features(coder_1)\n",
    "coder_3_features = automated_features(coder_3)\n",
    "coder_4_features = automated_features(coder_4)\n",
    "coder_5_features = automated_features(coder_5)\n",
    "\n",
    "coder_4_features['ner'] = pd.DataFrame({'ner': coder_4_ner})\n",
    "coder_5_features['ner'] = pd.DataFrame({'ner': coder_5_ner})\n",
    "coder_3_features['ner'] = pd.DataFrame({'ner': coder_3_ner})\n",
    "coder_1_features['ner'] = pd.DataFrame({'ner': coder_1_ner})\n",
    "coder_4_features['sen'] = pd.DataFrame({'sen': coder_4_sen})\n",
    "coder_5_features['sen'] = pd.DataFrame({'sen' : coder_5_sen})\n",
    "coder_1_features['sen'] = pd.DataFrame({'sen': coder_1_sen})   \n",
    "coder_3_features['sen'] = pd.DataFrame({'sen': coder_3_sen})\n",
    "coder_1_features['ner'] = coder_1_features['ner'].str.lower()\n",
    "coder_5_features['ner'] = coder_5_features['ner'].str.lower()\n",
    "coder_4_features['ner'] = coder_4_features['ner'].str.lower()\n",
    "coder_3_features['ner'] = coder_3_features['ner'].str.lower()\n",
    "#these datasets make up coders.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training set with all features from coders and the automated feature extraction\n",
    "#coders data set is available in the repository\n",
    "train_pd= pd.read_csv(\"coders.csv\", encoding = 'utf-8')\n",
    "stopwords = pd.read_csv(\"stopwords.csv\")\n",
    "def connect_frames(name, tag):\n",
    "    name = pd.read_csv('{}_features.csv'.format(name), sep = ',', encoding = 'utf-8')\n",
    "    frames = train_pd\n",
    "    frames = frames.loc[(frames['Q3'] == 'Machine set') & (frames['Q1'] == '{}'.format(tag))]\n",
    "    frames = frames[['Q2','A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
    "    frames['Q2'] = frames['Q2'].apply(int)\n",
    "    name['Q2'] = name.index\n",
    "    new = pd.merge(name,frames, how = 'left', on = 'Q2')\n",
    "    return new\n",
    "#create all feature data set from all \n",
    "coder_1 = connect_frames('coder_1', 'cd_1')\n",
    "coder_3 = connect_frames('coder_3', 'cd_3')\n",
    "coder_4 = connect_frames('coder_4', 'cd_4')\n",
    "coder_5 = connect_frames('coder_5', 'cd_5')\n",
    "train = pd.concat([coder_1,coder_3, coder_4, coder_5], axis=0, ignore_index=True)\n",
    "train = train[['text', 'country-source', 'Q2', 'D', 'E', 'F', 'G', 'article_len', 'ratio_not_stop_w', 'avg_word_len', 'title_len', 'ratio_unique_words', 'ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same functions are applied to the entire data set remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data = test_data[test_data.text.apply(lambda x: isinstance(x,str))]\n",
    "test = automated_features(test_data)\n",
    "test_ner = ner(test)\n",
    "test['ner'] = pd.DataFrame({'ner' : test_ner})\n",
    "test.to_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Krippendorf's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "coders = pd.read_csv('coders.csv')\n",
    "tags = ['cd_1', 'cd_3', 'cd_4', 'cd_5']\n",
    "coders = coders.drop(1713)#remove \n",
    "coders = coders.drop(1740)#remove\n",
    "coders = coders.loc[coders['Q3'] == 'Rel2']\n",
    "def krip_alpha(name, tag, field):\n",
    "    name = coders[coders['Q1'] == '{}'.format(tag)]\n",
    "    name = pd.DataFrame(name['{}'.format(field)])\n",
    "    name['{}'.format(field)] = np.where(name['{}'.format(field)]== 'Yes', 1, 0)\n",
    "    name_list = name['{}'.format(field)].tolist()\n",
    "    return name_list\n",
    "coders = pd.read_csv('coders.csv')\n",
    "coders = coders.loc[coders['Q3'] == 'Machine set']\n",
    "len(coders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values = []\n",
    "for tag in tags:\n",
    "    coder = krip_alpha('name', tag, 'G')\n",
    "    k_values.append(coder)\n",
    "k.alpha(k_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
